---
title: "Lab 05"
author: "Biology Student"
date: "2/10/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading Libraries
Use the following syntax to load any libraries that you need. This code will prevent errors that will interfere with knitting of the Rmd file.
Also note that the `include=FALSE` option will prevent this code chunk from appearing in the markdown file.

We will load the entire tidyverse library, which includes *dplyr*, *ggplot2*, *readr*, and other packages.

```{r Load Libraries, include=FALSE}
if (!require("tidyverse")) install.packages("tidyverse"); library(tidyverse)
```

## Objectives for Lab 5

1. Statistical tests using pairwise comparisons
2. Generalized linear models

The following exercise is adapted from http://www.biostathandbook.com/multiplecomparisons.html
McDonald, J.H. 2014. Handbook of Biological Statistics (3rd ed.). Sparky House Publishing, Baltimore, Maryland. 
and the R code from http://rcompanion.org/rcompanion/f_01.html

## Background

García-Arenzana et al. (2014) tested associations of 25 dietary variables with mammographic density, an important risk factor for breast cancer, in Spanish women. 
García-Arenzana, N., E.M. Navarrete-Muñoz, V. Lope, P. Moreo, S. Laso-Pablos, N. Ascunce, F. Casanova-Gómez, C. Sánchez-Contador, C. Santamariña, N. Aragonés, B.P. Gómez, J. Vioque, and M. Pollán. 2014. Calorie intake, olive oil consumption and mammographic density among Spanish women. International journal of cancer 134: 1916-1925.

```{r}
Data = read.table(textConnection("
Food               p
Blue_fish         .34
Bread             .594
Butter            .212
Carbohydrates     .384
Cereals_and_pasta .074
Dairy_products    .94
Eggs              .275
Fats              .696
Fruit             .269
Legumes           .341
Nuts              .06
Olive_oil         .008
Potatoes          .569
Processed_meat    .986
Proteins           .042
Red_meat           .251
Semi-skimmed_milk  .942
Skimmed_milk       .222
Sweets             .762
Total_calories     .001
Total_meat         .975
Vegetables         .216
White_fish         .205
White_meat         .041
Whole_milk         .039
"),header=TRUE)
```

## Unadjusted p Values

Use dplyr to arrange the data frame in order of p values.
Use dplyr to filter the data frame for p values < 0.05.

```{r}

```

## Bonferroni Correction

As you can see, five of the variables show a significant (P<0.05) P value. 
However, because García-Arenzana et al. (2014) tested 25 dietary variables, you'd expect one or two variables to show a significant result purely by chance, even if diet had no real effect on mammographic density. 
Applying the Bonferroni correction, you'd divide P=0.05 by the number of tests (25) to get the Bonferroni critical value, so a test would have to have P<0.002 to be significant. 
Under that criterion, only the test for total calories is significant.

To apply the Bonferroni correction to this table, you can use the function p.adjust().
Use dplyr to mutate the data frame, adding a column for Bonferroni-adjusted p values.
Now which dietary variables are significant?

```{r}

```

## Limitations of the Bonferroni Correction

The Bonferroni correction is appropriate when a single false positive in a set of tests would be a problem. It is mainly useful when there are a fairly small number of multiple comparisons and you're looking for one or two that might be significant. However, if you have a large number of multiple comparisons and you're looking for many that might be significant, the Bonferroni correction may lead to a very high rate of false negatives. For example, let's say you're comparing the expression level of 20,000 genes between liver cancer tissue and normal liver tissue. Based on previous studies, you are hoping to find dozens or hundreds of genes with different expression levels. If you use the Bonferroni correction, a P value would have to be less than 0.05/20000=0.0000025 to be significant. Only genes with huge differences in expression will have a P value that low, and could miss out on a lot of important differences just because you wanted to be sure that your results did not include a single false positive.

Another important issue with the Bonferroni correction is deciding what a "family" of statistical tests is. García-Arenzana et al. (2014) tested 25 dietary variables, so are these tests one "family," making the critical P value 0.05/25? But they also measured 13 non-dietary variables such as age, education, and socioeconomic status; should they be included in the family of tests, making the critical P value 0.05/38? And what if in 2015, García-Arenzana et al. write another paper in which they compare 30 dietary variables between breast cancer and non-breast cancer patients; should they include those in their family of tests, and go back and reanalyze the data in their 2014 paper using a critical P value of 0.05/55? There is no firm rule on this; you'll have to use your judgment, based on just how bad a false positive would be. Obviously, you should make this decision before you look at the results, otherwise it would be too easy to subconsciously rationalize a family size that gives you the results you want.

## False discovery rate: Benjamini–Hochberg procedure

An alternative approach is to control the false discovery rate. This is the proportion of "discoveries" (significant results) that are actually false positives. For example, let's say you're using microarrays to compare expression levels for 20,000 genes between liver tumors and normal liver cells. You're going to do additional experiments on any genes that show a significant difference between the normal and tumor cells, and you're willing to accept up to 10% of the genes with significant results being false positives; you'll find out they're false positives when you do the followup experiments. In this case, you would set your false discovery rate to 10%.

One good technique for controlling the false discovery rate was briefly mentioned by Simes (1986) and developed in detail by Benjamini and Hochberg (1995). Put the individual P values in order, from smallest to largest. The smallest P value has a rank of i=1, then next smallest has i=2, etc. Compare each individual P value to its Benjamini-Hochberg critical value, (i/m)Q, where i is the rank, m is the total number of tests, and Q is the false discovery rate you choose. The largest P value that has P<(i/m)Q is significant, and all of the P values smaller than it are also significant, even the ones that aren't less than their Benjamini-Hochberg critical value.

Simes, R.J. 1986. An improved Bonferroni procedure for multiple tests of significance. Biometrika 73: 751-754.

Benjamini, Y., and Y. Hochberg. 1995. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society B 57: 289-300.

Use dplyr to mutate the data frame, adding a column for Benjamini-Hochberg-adjusted p values. 
This can be called using either the method "BH" or "fdr" in the p.adjust function.
Now which dietary variables are significant at an FDR cutoff of 0.05? At an FDR cutoff of 0.25? 
How do the adjusted p-values for the BH procedure compare to the Bonferroni procedure?

```{r}

```

You should carefully choose your false discovery rate before collecting your data. Usually, when you're doing a large number of statistical tests, your experiment is just the first, exploratory step, and you're going to follow up with more experiments on the interesting individual results. If the cost of additional experiments is low and the cost of a false negative (missing a potentially important discovery) is high, you should probably use a fairly high false discovery rate, like 0.10 or 0.20, so that you don't miss anything important. Sometimes people use a false discovery rate of 0.05, probably because of confusion about the difference between false discovery rate and probability of a false positive when the null is true; a false discovery rate of 0.05 is probably too low for many experiments.

The Benjamini-Hochberg procedure is less sensitive than the Bonferroni procedure to your decision about what is a "family" of tests. If you increase the number of tests, and the distribution of P values is the same in the newly added tests as in the original tests, the Benjamini-Hochberg procedure will yield the same proportion of significant results. For example, if García-Arenzana et al. (2014) had looked at 50 variables instead of 25 and the new 25 tests had the same set of P values as the original 25, they would have 10 significant results under Benjamini-Hochberg with a false discovery rate of 0.25. This doesn't mean you can completely ignore the question of what constitutes a family; if you mix two sets of tests, one with some low P values and a second set without low P values, you will reduce the number of significant results compared to just analyzing the first set by itself.

## Limitations of Benjamini-Hochberg

The Bonferroni correction and Benjamini-Hochberg procedure assume that the individual tests are independent of each other, as when you are comparing sample A vs. sample B, C vs. D, E vs. F, etc. If you are comparing sample A vs. sample B, A vs. C, A vs. D, etc., the comparisons are not independent; if A is higher than B, there's a good chance that A will be higher than C as well. An example of an experimental design with multiple, non-independent comparisons is when you compare multiple variables between groups, and the variables are correlated with each other within groups. An example would be knocking out your favorite gene in mice and comparing everything you can think of on knockout vs. control mice: length, weight, strength, running speed, food consumption, feces production, etc. All of these variables are likely to be correlated within groups; mice that are longer will probably also weigh more, would be stronger, run faster, eat more food, and poop more. To analyze this kind of experiment, you can use a generalized linear model.

## When not to correct for multiple comparisons

The goal of multiple comparisons corrections is to reduce the number of false positives, because false positives can be embarrassing, confusing, and cause you and other people to waste your time. An unfortunate byproduct of correcting for multiple comparisons is that you may increase the number of false negatives, where there really is an effect but you don't detect it as statistically significant. If false negatives are very costly, you may not want to correct for multiple comparisons at all. For example, let's say you've gone to a lot of trouble and expense to knock out your favorite gene, mannose-6-phosphate isomerase (Mpi), in a strain of mice that spontaneously develop lots of tumors. Hands trembling with excitement, you get the first Mpi-/- mice and start measuring things: blood pressure, growth rate, maze-learning speed, bone density, coat glossiness, everything you can think of to measure on a mouse. You measure 50 things on Mpi-/- mice and normal mice, run the approriate statistical tests, and the smallest P value is 0.013 for a difference in tumor size. If you use a Bonferroni correction, that P=0.013 won't be close to significant; it might not be significant with the Benjamini-Hochberg procedure, either. Should you conclude that there's no significant difference between the Mpi-/- and Mpi+/+ mice, write a boring little paper titled "Lack of anything interesting in Mpi-/- mice," and look for another project? No, your paper should be "Possible effect of Mpi on cancer." You should be suitably cautious, of course, and emphasize in the paper that there's a good chance that your result is a false positive; but the cost of a false positive—if further experiments show that Mpi really has no effect on tumors—is just a few more experiments. The cost of a false negative, on the other hand, could be that you've missed out on a hugely important discovery.


# Generalized Linear Models

The msleep dataset has 11 variables:
name	= species common name
genus	= genus name
vore	= feeding type (carnivore/herbivore/omnivore/insectivore)
order	= name of the order
conservation	= the IUCN conservation status of the species (lc/nt/en/domesticated vu/cd)
sleep_total	= total sleep time (hours)
sleep_rem	= REM sleep time (hours)
sleep_cycle	= length of sleep cycle (hours)
awake	= time spent awake (hours)
brainwt	= brain weight (kg)
bodywt	= body weight (kg)

```{r}
if (!require("ggplot2")) install.packages("ggplot2"); library(ggplot2)
if (!require("cowplot")) install.packages("cowplot"); library(cowplot)
data(msleep)
str(msleep)
model <- glm(formula= sleep_rem ~ brainwt, data=msleep)
summary(model)
```

```{r}
ggplot(msleep) +
  aes(x = log10(bodywt), y = sleep_total) +
  geom_point(aes(color = factor(vore))) +
  geom_smooth(method=lm , color="black", se=TRUE) +
  theme_cowplot() +
  labs(color="Feeding Style") +
  xlab("Log body weight (kg)") + 
  ylab("Total sleep (hr/day)")
```

```{r Statistics on linear regression}
sleepfit = lm(sleep_total ~ log10(bodywt), data=msleep)
summary(sleepfit)
```

```{r simple GLM}
sleepglm = glm(sleep_total ~ log10(bodywt), data=msleep)
summary(sleepglm)
```

```{r full GLM}
sleepfullglm = glm(sleep_total ~ log10(bodywt) + log10(brainwt) + vore + order + conservation, data=msleep)
summary(sleepfullglm)
```

```{r}
if (!require("MASS")) install.packages("MASS"); library(MASS)
dropterm()
```


## Summary

The dplyr package provides a concise set of operations for managing data frames. 
With these functions we can do a number of complex operations in just a few lines of code. 
In particular, we can often conduct the beginnings of an exploratory analysis with the powerful combination of group_by() and summarize().